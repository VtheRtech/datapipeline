GROUP BY fy_declared
ORDER by fy_declared DESC;")
disaster_df <- as.tibble(dbReadTable(con, "us_disaster_declarations"))
type<- disaster_df %>%
count(incident_type)
head(disaster_df)
# Summary for Numeric Columns
numeric_summary <- disaster_df %>%
summarise(across(where(is.numeric), list(
mean = ~mean(., na.rm = TRUE),
median = ~median(., na.rm = TRUE),
min = ~min(., na.rm = TRUE),
max = ~max(., na.rm = TRUE),
n_missing = ~sum(is.na(.))
)))
# Summary for Character Columns
character_summary <- disaster_df %>%
summarise(across(where(is.character), list(
unique_count = ~n_distinct(.),
most_common = ~names(sort(table(.), decreasing = TRUE)[1]),
n_missing = ~sum(is.na(.))
)))
# Print summaries
print(numeric_summary)
print(character_summary)
res <- res %>%
rename(Number_of_Disasters_Declared = number_of_incidents,Year_Declared = fy_declared)
type <- type %>%
arrange(n)
boxplot(res$Number_of_Disasters_Declared)
boxplot(type$n)
res <- res %>%
filter(Number_of_Disasters_Declared < 4500,Year_Declared < 2023)
boxplot(res$Number_of_Disasters_Declared)
# Assuming disaster_df is your dataframe
# Reordering the factor based on count
disaster_df <- disaster_df %>%
mutate(incident_type = reorder(incident_type, incident_type, FUN = function(x) -length(x)))
# Plotting
ggplot(disaster_df, aes(x = incident_type)) +
geom_bar(fill = "steelblue") +
labs(title = "Frequency of Different Types of Disasters",
x = "Disaster Type",
y = "Count") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Assuming disaster_df is your dataframe
# Reordering states based on the count of disasters
disaster_df <- disaster_df %>%
group_by(state) %>%
mutate(Count = n()) %>%
ungroup() %>%
mutate(state = reorder(state, Count, FUN = function(x) -sum(x)))
# Plotting
ggplot(disaster_df, aes(x = state, fill = incident_type)) +
geom_bar() +
labs(title = "Distribution of Disaster Types by State",
x = "State",
y = "Count") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Linear regression analysis
model <- lm(Number_of_Disasters_Declared ~ Year_Declared, data = res)
# Summary of the model
summary(model)
ggplot(res, aes(x = Year_Declared, y = Number_of_Disasters_Declared)) +
geom_point() +  # Scatter plot points
geom_smooth(method = "lm", col = "blue") +  # Regression line
labs(title = "Regression Analysis",
x = "Year Declared",
y = "Number of Disasters Declared") +
theme_minimal()
# Assuming 'model' is your linear model
ggplot(data = res, aes(x = fitted(model), y = resid(model))) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(title = "Residuals vs Fitted Values",
x = "Fitted Values",
y = "Residuals") +
theme_minimal()
# Assuming disaster_df is your dataframe
# Reordering the factor based on count
disaster_df <- disaster_df %>%
mutate(incident_type = reorder(incident_type, incident_type, FUN = function(x) -length(x)))
# Plotting
ggplot(disaster_df, aes(x = incident_type)) +
geom_bar(fill = "steelblue") +
labs(title = "Frequency of Different Types of Disasters",
x = "Disaster Type",
y = "Count") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Assuming disaster_df is your dataframe
# Reordering states based on the count of disasters
disaster_df <- disaster_df %>%
group_by(state) %>%
mutate(Count = n()) %>%
ungroup() %>%
mutate(state = reorder(state, Count, FUN = function(x) -sum(x)))
# Plotting
ggplot(disaster_df, aes(x = state, fill = incident_type)) +
geom_bar() +
labs(title = "Distribution of Disaster Types by State",
x = "State",
y = "Count") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(disaster_df, aes(x = incident_type)) +
geom_bar(fill = "steelblue") +
labs(title = "Frequency of Different Types of Disasters",
x = "Disaster Type",
y = "Count") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(tidyverse)
library(RSQLite)
library(DBI)
library(ggplot2)
library(dplyr)
setwd("~/workbook")
con <- dbConnect(RSQLite::SQLite(),"Disaster_Data.db")
dbListTables(con)
dbListFields(con,"us_disaster_declarations")
res<- dbSendQuery(con,
"SELECT fy_declared,
count(incident_type) AS number_of_incidents
FROM us_disaster_declarations
WHERE fy_declared > 1960
GROUP BY fy_declared
ORDER by fy_declared DESC;")
res<- dbGetQuery(con,
"SELECT fy_declared,
count(incident_type) AS number_of_incidents
FROM us_disaster_declarations
WHERE fy_declared > 1960
GROUP BY fy_declared
ORDER by fy_declared DESC;")
disaster_df <- as.tibble(dbReadTable(con, "us_disaster_declarations"))
type<- disaster_df %>%
count(incident_type)
head(disaster_df)
# Summary for Numeric Columns
numeric_summary <- disaster_df %>%
summarise(across(where(is.numeric), list(
mean = ~mean(., na.rm = TRUE),
median = ~median(., na.rm = TRUE),
min = ~min(., na.rm = TRUE),
max = ~max(., na.rm = TRUE),
n_missing = ~sum(is.na(.))
)))
# Summary for Character Columns
character_summary <- disaster_df %>%
summarise(across(where(is.character), list(
unique_count = ~n_distinct(.),
most_common = ~names(sort(table(.), decreasing = TRUE)[1]),
n_missing = ~sum(is.na(.))
)))
# Print summaries
print(numeric_summary)
print(character_summary)
library(tidyverse)
library(maps)
library(tidyverse)
install.packages(maps)
library(maps)
install.packages("maps")
library(tidyverse)
install.packages("maps")
library(maps)
library(ggplot2)
install.packages("ggmap")
library(ggmap)
install.packages("mapproj")
library(mapproj)
install.packages("choroplethr")
install.packages("choroplethrZip")
library(choroplethr)
install.packages("choroplethr")
install.packages("choroplethrZip")
library(choroplethr)
library(choroplethr)
df <- structure(list(region = c("45001", "45002", "45003", "45011",
"45013", "45014", "45030", "45041", "45044", "45050", "45052",
"45069", "45111", "45140", "45147", "45150", "45174", "45202",
"45203", "45204", "45205", "45206", "45207", "45208", "45209",
"45210", "45211", "45212", "45213", "45214", "45215", "45216",
"45217", "45218", "45219", "45220", "45223", "45224", "45225",
"45226", "45227", "45229", "45230", "45231", "45232", "45233",
"45236", "45237", "45238", "45239", "45240", "45241", "45242",
"45243", "45244", "45246", "45247", "45248", "45249", "45250",
"45251", "45252", "45255", "45033", "45051"), value = c(3, 20,
1, 11, 155, 7, 28, 1, 2, 1, 3, 5, 1, 20, 1, 73, 1, 52, 26, 15,
106, 109, 52, 8, 55, 1, 185, 69, 30, 99, 104, 35, 35, 9, 20,
96, 51, 182, 75, 3, 82, 133, 60, 246, 42, 13, 79, 217, 128, 146,
125, 71, 30, 10, 19, 62, 24, 21, 31, 1, 51, 3, 28, 0, 0)), row.names = c(NA,
-65L), class = c("tbl_df", "tbl", "data.frame"))
fip=c(39061)
ohiocounty_map <-
map_data("county") %>%
subset(region == "ohio") %>%
mutate(County = str_to_sentence(subregion)) %>%
group_by(County) %>%
filter(County %in% c("Hamilton"))
# get zip codes
data("zip.map")
myzips <- zip.map %>%
filter(region %in% df$region) %>%
rename(zipcode = region) %>%
group_by(zipcode) %>%
summarise(across(everything(), mean)) # without this line, there would be a lot of zip code labels
```data("zip.map")
myzips <- zip.map %>%
filter(region %in% df$region) %>%
rename(zipcode = region) %>%
group_by(zipcode)
myzips <- myzips %>%
summarise(across(where(is.numeric), mean, na.rm = TRUE))
myzips <- myzips %>%
summarise(across(where(is.numeric), mean, na.rm = TRUE))
install.packages("choroplethrZip")
library(tidyr)
library(dplyr)
# Revised dataset
book_ratings <- tibble(
book_id = rep(1:4, each = 3),
month = c(1, 2, 4, 1, 3, 4, 1, 3, NA, 2, 4, NA),
rating = c(4, NA, 5, 3, NA, 4, NA, 5, 4, 3, NA, 4)
)
library(tidyr)
library(dplyr)
# Revised dataset
book_ratings <- tibble(
book_id = rep(1:4, each = 3),
month = c(1, 2, 4, 1, 3, 4, 1, 3, NA, 2, 4, NA),
rating = c(4, NA, 5, 3, NA, 4, NA, 5, 4, 3, NA, 4)
)
# Complete missing months
book_ratings_complete <- book_ratings %>%
complete(book_id, month = 1:4)
# Fill missing ratings
book_ratings_filled <- book_ratings_complete %>%
group_by(book_id) %>%  # Ensure filling is done within each book
fill(rating) %>%
ungroup()
# Now, drop rows with NA (if this step is still required after filling)
book_ratings_no_na <- book_ratings_filled %>%
drop_na()
book_ratings %>%
filter()
book_ratings %>%
filter(month < 2)
install.packages("httpgd")
install.packages("httpgd")
install.packages("httpgd")
options(reactable.theme = reactable::reactableTheme(
color = "hsl(233, 9%, 87%)",
backgroundColor = "#002b36",
borderColor = "#eee8d5",
stripedColor = "#586e75",
highlightColor = "#6c71c4",
inputStyle = list(backgroundColor = "hsl(233, 9%, 25%)"),
selectStyle = list(backgroundColor = "hsl(233, 9%, 25%)"),
pageButtonHoverStyle = list(backgroundColor = "hsl(233, 9%, 25%)"),
pageButtonActiveStyle = list(backgroundColor = "hsl(233, 9%, 28%)")
))
options(reactable.theme = reactable::reactableTheme(
color = "hsl(233, 9%, 87%)",
backgroundColor = "#002b36",
borderColor = "#eee8d5",
stripedColor = "#586e75",
highlightColor = "#6c71c4",
inputStyle = list(backgroundColor = "hsl(233, 9%, 25%)"),
selectStyle = list(backgroundColor = "hsl(233, 9%, 25%)"),
pageButtonHoverStyle = list(backgroundColor = "hsl(233, 9%, 25%)"),
pageButtonActiveStyle = list(backgroundColor = "hsl(233, 9%, 28%)")
))
options(reactable.theme = reactable::reactableTheme(
color = "hsl(233, 9%, 87%)",
backgroundColor = "#002b36",
borderColor = "#eee8d5",
stripedColor = "#586e75",
highlightColor = "#6c71c4",
inputStyle = list(backgroundColor = "hsl(233, 9%, 25%)"),
selectStyle = list(backgroundColor = "hsl(233, 9%, 25%)"),
pageButtonHoverStyle = list(backgroundColor = "hsl(233, 9%, 25%)"),
pageButtonActiveStyle = list(backgroundColor = "hsl(233, 9%, 28%)")
))
install.packages('reactable')
options(reactable.theme = reactable::reactableTheme(
color = "hsl(233, 9%, 87%)",
backgroundColor = "#002b36",
borderColor = "#eee8d5",
stripedColor = "#586e75",
highlightColor = "#6c71c4",
inputStyle = list(backgroundColor = "hsl(233, 9%, 25%)"),
selectStyle = list(backgroundColor = "hsl(233, 9%, 25%)"),
pageButtonHoverStyle = list(backgroundColor = "hsl(233, 9%, 25%)"),
pageButtonActiveStyle = list(backgroundColor = "hsl(233, 9%, 28%)")
))
view <- function(.data){
if(interactive()) {
reactable::reactable(.data,
filterable = TRUE,
searchable = TRUE,
showPageSizeOptions = TRUE,
striped = TRUE,
highlight = TRUE,
compact = TRUE,
defaultPageSize = 30)
}
}
view_xl <- function(.data){
if(interactive()){
tmp <- tempfile(fileext = ".csv")
data.table::fwrite(.data, tmp)
browseURL(tmp, browser = "gnumeric")
}
}
library(reactable)
options(reactable.theme = reactable::reactableTheme(
color = "hsl(233, 9%, 87%)",
backgroundColor = "#002b36",
borderColor = "#eee8d5",
stripedColor = "#586e75",
highlightColor = "#6c71c4",
inputStyle = list(backgroundColor = "hsl(233, 9%, 25%)"),
selectStyle = list(backgroundColor = "hsl(233, 9%, 25%)"),
pageButtonHoverStyle = list(backgroundColor = "hsl(233, 9%, 25%)"),
pageButtonActiveStyle = list(backgroundColor = "hsl(233, 9%, 28%)")
))
view <- function(.data){
if(interactive()) {
reactable::reactable(.data,
filterable = TRUE,
searchable = TRUE,
showPageSizeOptions = TRUE,
striped = TRUE,
highlight = TRUE,
compact = TRUE,
defaultPageSize = 30)
}
}
view_xl <- function(.data){
if(interactive()){
tmp <- tempfile(fileext = ".csv")
data.table::fwrite(.data, tmp)
browseURL(tmp, browser = "gnumeric")
}
}
iris
view(iris)
View(iris)
view(iris)
view_xl(iris)
view_xl(iris)
.rs.restartR()
.rs.restartR()
View(view)
View(view_xl)
View(view)
install.packages("nvimcom")
install.packages("htmlwidgets")
install.packages("htmlwidgets")
test <- iris
view(iris)
view(test)
View(view_xl)
View(view)
View(view)
# Created by use_targets().
# Follow the comments below to fill in this target script.
# Then follow the manual to check and run the pipeline:
#   https://books.ropensci.org/targets/walkthrough.html#inspect-the-pipeline
# Load packages required to define the pipeline:
install.packages("targets")
library(targets)
# library(tarchetypes) # Load other packages as needed.
# Set target options:
tar_option_set(
packages = c("dplyr", "ggplot2", "readr") # packages that your targets use
# format = "qs", # Optionally set the default storage format. qs is fast.
#
# Pipelines that take a long time to run may benefit from
# optional distributed computing. To use this capability
# in tar_make(), supply a {crew} controller
# as discussed at https://books.ropensci.org/targets/crew.html.
# Choose a controller that suits your needs. For example, the following
# sets a controller that scales up to a maximum of two workers
# which run as local R processes. Each worker launches when there is work
# to do and exits if 60 seconds pass with no tasks to run.
#
#   controller = crew::crew_controller_local(workers = 2, seconds_idle = 60)
#
# Alternatively, if you want workers to run on a high-performance computing
# cluster, select a controller from the {crew.cluster} package.
# For the cloud, see plugin packages like {crew.aws.batch}.
# The following example is a controller for Sun Grid Engine (SGE).
#
#   controller = crew.cluster::crew_controller_sge(
#     # Number of workers that the pipeline can scale up to:
#     workers = 10,
#     # It is recommended to set an idle time so workers can shut themselves
#     # down if they are not running tasks.
#     seconds_idle = 120,
#     # Many clusters install R as an environment module, and you can load it
#     # with the script_lines argument. To select a specific verison of R,
#     # you may need to include a version string, e.g. "module load R/4.3.2".
#     # Check with your system administrator if you are unsure.
#     script_lines = "module load R"
#   )
#
# Set other options as needed.
)
# Run the R scripts in the R/ folder with your custom functions:
tar_source()
tar_visnetwork()
library(targets)
# library(tarchetypes) # Load other packages as needed.
# Set target options:
tar_option_set(
packages = c("dplyr", "ggplot2", "readr") # packages that your targets use
# format = "qs", # Optionally set the default storage format. qs is fast.
#
# Pipelines that take a long time to run may benefit from
# optional distributed computing. To use this capability
# in tar_make(), supply a {crew} controller
# as discussed at https://books.ropensci.org/targets/crew.html.
# Choose a controller that suits your needs. For example, the following
# sets a controller that scales up to a maximum of two workers
# which run as local R processes. Each worker launches when there is work
# to do and exits if 60 seconds pass with no tasks to run.
#
#   controller = crew::crew_controller_local(workers = 2, seconds_idle = 60)
#
# Alternatively, if you want workers to run on a high-performance computing
# cluster, select a controller from the {crew.cluster} package.
# For the cloud, see plugin packages like {crew.aws.batch}.
# The following example is a controller for Sun Grid Engine (SGE).
#
#   controller = crew.cluster::crew_controller_sge(
#     # Number of workers that the pipeline can scale up to:
#     workers = 10,
#     # It is recommended to set an idle time so workers can shut themselves
#     # down if they are not running tasks.
#     seconds_idle = 120,
#     # Many clusters install R as an environment module, and you can load it
#     # with the script_lines argument. To select a specific verison of R,
#     # you may need to include a version string, e.g. "module load R/4.3.2".
#     # Check with your system administrator if you are unsure.
#     script_lines = "module load R"
#   )
#
# Set other options as needed.
)
# Run the R scripts in the R/ folder with your custom functions:
tar_source()
get_data <- function(file) {
read_csv(file, col_types = cols()) %>%
filter(!is.na(Ozone))
}
fit_model <- function(data) {
lm(Ozone ~ Temp, data) %>%
coefficients()
}
plot_model <- function(model, data) {
ggplot(data) +
geom_point(aes(x = Temp, y = Ozone)) +
geom_abline(intercept = model[1], slope = model[2]) +
theme_gray(24)
}
# Set target options:
tar_option_set(
packages = c("dplyr", "ggplot2", "readr") # packages that your targets use
# format = "qs", # Optionally set the default storage format. qs is fast.
#
# Pipelines that take a long time to run may benefit from
# optional distributed computing. To use this capability
# in tar_make(), supply a {crew} controller
# as discussed at https://books.ropensci.org/targets/crew.html.
# Choose a controller that suits your needs. For example, the following
# sets a controller that scales up to a maximum of two workers
# which run as local R processes. Each worker launches when there is work
# to do and exits if 60 seconds pass with no tasks to run.
#
#   controller = crew::crew_controller_local(workers = 2, seconds_idle = 60)
#
# Alternatively, if you want workers to run on a high-performance computing
# cluster, select a controller from the {crew.cluster} package.
# For the cloud, see plugin packages like {crew.aws.batch}.
# The following example is a controller for Sun Grid Engine (SGE).
#
#   controller = crew.cluster::crew_controller_sge(
#     # Number of workers that the pipeline can scale up to:
#     workers = 10,
#     # It is recommended to set an idle time so workers can shut themselves
#     # down if they are not running tasks.
#     seconds_idle = 120,
#     # Many clusters install R as an environment module, and you can load it
#     # with the script_lines argument. To select a specific verison of R,
#     # you may need to include a version string, e.g. "module load R/4.3.2".
#     # Check with your system administrator if you are unsure.
#     script_lines = "module load R"
#   )
#
# Set other options as needed.
)
# Run the R scripts in the R/ folder with your custom functions:
tar_source()
install.packages(‘httpgd’)
install.packages(httpgd)
install.packages(tidyverse)
install.packages("httpgd")
nvimcom:::source.and.clean("/home/pretender/Lab4/_targets.R.tmp.R", echo=TRUE)
nvimcom:::source.and.clean("/home/pretender/Lab4/functions/functions.R.tmp.R", echo=TRUE)
nvimcom:::source.and.clean("/home/pretender/Lab4/_targets.R.tmp.R", echo=TRUE)
tar_make()
quit(save = "yes")
